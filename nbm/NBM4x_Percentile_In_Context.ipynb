{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/NBM_4_1_Percentile_In_Context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7inkOnFayzV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4uThJiMbzHo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **RUN:** This first cell imports condaColab. <br><br>\n",
        "#@markdown **NOTE: This cell will restart the notebook, which will prompt a crash popup in the lower left corner. This is safe to ignore and move on once the notebook comes back up** (you will see RAM and Disk in the upper right corner).\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwcXy34Yb3br",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@markdown **RUN:** This second cell imports our python modules\n",
        "\n",
        "# Install required packages\n",
        "!mamba install -q -c conda-forge cartopy contextily pyepsg pygrib netCDF4\n",
        "\n",
        "from IPython.display import Javascript\n",
        "\n",
        "# General libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "import zipfile\n",
        "import itertools\n",
        "import requests\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "\n",
        "# Geospatial and projections\n",
        "import pygrib\n",
        "import geopandas as gpd\n",
        "from pyproj import Proj, transform\n",
        "\n",
        "# Plotting\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cartopy and contextily for maps\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.io.shapereader import Reader\n",
        "from cartopy.feature import ShapelyFeature\n",
        "\n",
        "# Matplotlib configuration\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Liberation Sans'\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #<b>Options Selection { display-mode: \"form\" }\n",
        "synoptic_token = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "element = \"maxt\" #@param [\"maxt\", \"mint\",\"qpf\"]\n",
        "valid_date = \"2025-03-25\" #@param {type:\"date\"}\n",
        "#@markdown ##QPF valid 24 hours ending time\n",
        "qpf_valid_time = 6 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "##@markdown ##Use StageIV at stations instead of obs?\n",
        "use_stageiv = False\n",
        "##@param {type:\"boolean\"}\n",
        "#@markdown ##Pick NBM run time\n",
        "nbm_init_date = \"2025-03-24\" #@param {type:\"date\"}\n",
        "nbm_init_hour = 12 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "#@markdown ##Where do you want to focus?\n",
        "# @markdown <FONT SIZE=5>**5. For Which Region?**\n",
        "region_selection = \"WR\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"CONUS\", \"CWA\", \"RFC\"]\n",
        "#@markdown If CWA/RFC selected, which one? (i.e. \"SLC\" for Salt Lake City, \"CBRFC\" for Colorado Basin)\n",
        "cwa_selection = '' #@param {type:\"string\"}\n",
        "# compare_to = \"deterministic\" #@param [\"obs\", \"deterministic\"]\n",
        "#@markdown ##Which obs?\n",
        "network_selection = \"NWS\" #@param [\"NWS\", \"RAWS\", \"HADS\", \"SNOTEL\", \"NWS+RAWS\", \"NWS+RAWS+HADS\", \"ALL\", \"CUSTOM\", \"LIST\"]\n",
        "#@markdown ##Elevation selection (ft AMSL) <br>\n",
        "#@markdown (Warning: Setting lower >= upper or upper <= lower will reset to default of 0-15000)\n",
        "elev_lower_limit = 0 #@param {type:\"slider\", min:0, max:15000, step:250}\n",
        "elev_upper_limit = 15000 #@param {type:\"slider\", min:0, max:15000, step:250}\n",
        "#@markdown ##If Custom or List selected for network:\n",
        "#@markdown Enter comma separated network IDs (custom) or siteids (list)  WITH NO SPACES here. For help - https://developers.synopticdata.com/about/station-providers/\n",
        "network_input = \"\"#@param {type:\"string\"}\n",
        "# cwa_outline = True #@param {type:\"boolean\"}\n",
        "cwa_outline = False\n",
        "#@markdown ##Do you want a CSV?\n",
        "export_csv = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Setup a dictionary for translating a form selection into a something we can pass to mesowest API\n",
        "network_dict = {\"NWS+RAWS+HADS\":\"&network=1,2,106\",\n",
        "                \"NWS+RAWS\":\"&network=1,2\",\n",
        "                \"NWS\":\"&network=1\",\n",
        "                \"RAWS\": \"&network=2\",\n",
        "                \"HADS\": \"&network=106\",\n",
        "                \"SNOTEL\": \"&network=25\",\n",
        "                \"ALL\":\"\",\n",
        "                \"CUSTOM\": \"&network=\"+network_input,\n",
        "                \"LIST\": \"&stid=\"+network_input}\n",
        "\n",
        "network_string = network_dict[network_selection]\n",
        "\n",
        "def cwa_list(input_region):\n",
        "  region_dict ={\"WR\":\"BYZ,BOI,LKN,EKA,FGZ,GGW,TFX,VEF,LOX,MFR,MTR,MSO,PDT,PSR,PIH,PQR,REV,STO,SLC,SGX,HNX,SEW,OTX,TWC\",\n",
        "              \"CR\":\"ABR,BIS,CYS,LOT,DVN,BOU,DMX,DTX,DDC,DLH,FGF,GLD,GJT,GRR,GRB,GID,IND,JKL,EAX,ARX,ILX,LMK,MQT,MKX,MPX,LBF,APX,IWX,OAX,PAH,PUB,UNR,RIW,FSD,SGF,LSX,TOP,ICT\",\n",
        "              \"ER\":\"ALY,LWX,BGM,BOX,BUF,BTV,CAR,CTP,RLX,CHS,ILN,CLE,CAE,GSP,MHX,OKX,PHI,PBZ,GYX,RAH,RNK,AKQ,ILM\",\n",
        "              \"SR\":\"ABQ,AMA,FFC,EWX,BMX,BRO,CRP,EPZ,FWD,HGX,HUN,JAN,JAX,KEY,MRX,LCH,LZK,LUB,MLB,MEG,MFL,MOB,MAF,OHX,LIX,OUN,SJT,SHV,TAE,TBW,TSA\"}\n",
        "  if (input_region in [\"WR\", \"CR\", \"SR\", \"ER\"]):\n",
        "    cwas_list = region_dict[input_region]\n",
        "  else:\n",
        "    cwas_list = input_region\n",
        "  return cwas_list\n",
        "\n",
        "def cwa_list_rfc(input_rfc):\n",
        "\n",
        "    metadata_api = 'https://api.synopticdata.com/v2/stations/metadata?'\n",
        "\n",
        "    network_query = (f\"{network_dict[network_selection]}\"\n",
        "                    if network_dict[network_selection] is not None else '')\n",
        "\n",
        "    # Assemble the API query\n",
        "    api_query = (f\"{metadata_api}&token={synoptic_token}\" + network_query +\n",
        "                f\"&complete=1&sensorvars=1,obrange=20230118\") #hardcoded for NBM4.1+\n",
        "\n",
        "    # Print the API query to output\n",
        "    print(api_query)\n",
        "\n",
        "    # Get the data from the API\n",
        "    response = requests.get(api_query)\n",
        "    metadata = pd.DataFrame(response.json()['STATION'])\n",
        "\n",
        "    # Remove NaNs and index by network, station ID\n",
        "    metadata = metadata[metadata['MNET_SHORTNAME'].notna()]\n",
        "    metadata = metadata.set_index(['MNET_SHORTNAME', 'STID'])\n",
        "\n",
        "    metadata['LATITUDE'] = metadata['LATITUDE'].astype(float)\n",
        "    metadata['LONGITUDE'] = metadata['LONGITUDE'].astype(float)\n",
        "    metadata['ELEVATION'] = metadata['ELEVATION'].astype(float)\n",
        "\n",
        "    metadata = metadata[metadata['LATITUDE'] >= 31]\n",
        "    metadata = metadata[metadata['LONGITUDE'] <= -103.00]\n",
        "    metadata = metadata[metadata['STATUS'] == 'ACTIVE']\n",
        "\n",
        "    geometry = gpd.points_from_xy(metadata.LONGITUDE, metadata.LATITUDE)\n",
        "    metadata = gpd.GeoDataFrame(metadata, geometry=geometry)\n",
        "\n",
        "    req = requests.get(\n",
        "        'https://www.weather.gov/source/gis/Shapefiles/WSOM/w_05mr24.zip',\n",
        "\n",
        "    allow_redirects=True)\n",
        "    open('w_05mr24.zip', 'wb').write(req.content)\n",
        "\n",
        "    with zipfile.ZipFile('w_05mr24.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    rfc_shp = gpd.read_file('w_05mr24.shp').set_index('BASIN_ID')\n",
        "\n",
        "    metadata = metadata[metadata.geometry.within(rfc_shp.geometry.loc[input_rfc])]\n",
        "\n",
        "    rfc_site_list = metadata.index.get_level_values(1).unique()\n",
        "    rfc_cwa_list = metadata['CWA'].unique()\n",
        "\n",
        "    return metadata\n",
        "\n",
        "if ((elev_lower_limit >= elev_upper_limit)\n",
        "        or (elev_upper_limit <= elev_lower_limit)):\n",
        "    elev_lower_limit = 0\n",
        "    elev_upper_limit = 15000\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  region_list = [\"WR\", \"CR\", \"SR\", \"ER\"]\n",
        "elif region_selection == \"CWA\":\n",
        "  region_list = [cwa_selection]\n",
        "elif region_selection == \"RFC\":\n",
        "  rfc_metadata = cwa_list_rfc(cwa_selection)\n",
        "#   cwa_query = ','.join([str(cwa) for cwa in rfc_metadata['CWA'].unique()\n",
        "#     if cwa is not None])\n",
        "  region_list = rfc_metadata['CWA'].unique()\n",
        "  region_list = [x for x in region_list if x is not None]\n",
        "else:\n",
        "  region_list = [region_selection]\n",
        "\n",
        "nbm_init = datetime.strptime(nbm_init_date,'%Y-%m-%d') + timedelta(hours=int(nbm_init_hour))\n",
        "\n",
        "if element == \"maxt\":\n",
        "    nbm_core_valid_hour=\"00\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    obs_start_hour = \"1200\"\n",
        "    obs_end_hour = \"0600\"\n",
        "    ob_stat = \"maximum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"mint\":\n",
        "    nbm_core_valid_hour=\"12\"\n",
        "    nbm_qmd_valid_hour=\"18\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    obs_start_hour = \"0000\"\n",
        "    obs_end_hour = \"1800\"\n",
        "    ob_stat = \"minimum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"qpf\":\n",
        "    nbm_core_valid_hour = str(qpf_valid_time)\n",
        "    nbm_valid_hour = str(qpf_valid_time)\n",
        "    nbm_qmd_valid_hour=str(qpf_valid_time)\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    obs_start_hour = f'{qpf_valid_time:02d}00' #str(qpf_valid_time)+\"00\"\n",
        "    obs_end_hour = f'{qpf_valid_time:02d}00' #str(qpf_valid_time)+\"00\"\n",
        "    ob_stat = \"total\"\n",
        "    valid_end_datetime = valid_date_end\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - nbm_init\n",
        "\n",
        "current_datetime = datetime.now()\n",
        "\n",
        "nbm_core_forecasthour = nbm_core_fhdelta.total_seconds() / 3600.\n",
        "nbm_core_forecasthour_start = nbm_core_forecasthour - 12\n",
        "nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "if element == \"qpf\":\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 24\n",
        "else:\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 18\n",
        "\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/legacystats?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "\n",
        "if element == \"qpf\":\n",
        "  cmap = 'BrBG_r'\n",
        "  txcol = 'white'\n",
        "else:\n",
        "  cmap = 'Spectral'\n",
        "  txcol = 'black'\n",
        "if use_stageiv and element==\"qpf\":\n",
        "  points_str = f'Stage IV @ {network_selection}'\n",
        "else:\n",
        "  points_str = network_selection\n",
        "\n",
        "########################################################################################################################\n",
        "# Reusable functions section                                                                                           #\n",
        "########################################################################################################################\n",
        "\n",
        "def project3(lon, lat, prj):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  outproj = prj\n",
        "  inproj = Proj(init='epsg:4326')\n",
        "  nbm_coords = transform(inproj, outproj, lon, lat)\n",
        "  coordX = nbm_coords[0]\n",
        "  coordY = nbm_coords[1]\n",
        "  #print(f'Lat: {lat}, Y: {coordY} | Lon: {lon}, X: {coordX}')\n",
        "  return(coordX, coordY)\n",
        "\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "\n",
        "def project_hrap(lon, lat, s4x, s4y):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  globe = ccrs.Globe(semimajor_axis=6371200)\n",
        "  hrap_ccrs = proj = ccrs.Stereographic(central_latitude=90.0,\n",
        "                          central_longitude=255.0,\n",
        "                          true_scale_latitude=60.0, globe=globe)\n",
        "  latlon_ccrs = ccrs.PlateCarree()\n",
        "  hrap_coords = hrap_ccrs.transform_point(lon,lat,src_crs=latlon_ccrs)\n",
        "  hrap_idx = ll_to_index(s4x, s4y, hrap_coords[0], hrap_coords[1])\n",
        "\n",
        "  return hrap_idx\n",
        "\n",
        "\n",
        "def get_stageiv():\n",
        "  siv_url = \"https://water.weather.gov/precip/downloads/\"+valid_date_end.strftime('%Y')+\"/\"+valid_date_end.strftime('%m')+\"/\"+valid_date_end.strftime('%d')+\"/nws_precip_1day_\"+valid_date_end.strftime('%Y%m%d')+\"_conus.nc\"\n",
        "  data = urlopen(siv_url).read()\n",
        "\n",
        "  nc = Dataset('data', memory=data)\n",
        "  #with Dataset(siv_file, 'r') as nc:\n",
        "  stageIV = nc.variables['observation']\n",
        "  s4x = nc.variables['x']\n",
        "  s4y = nc.variables['y']\n",
        "  return stageIV, s4x, s4y\n",
        "\n",
        "\n",
        "def K_to_F(kelvin):\n",
        "  fahrenheit = 1.8*(kelvin-273)+32.\n",
        "  return fahrenheit\n",
        "\n",
        "\n",
        "def mm_to_in(millimeters):\n",
        "  inches = millimeters * 0.0393701\n",
        "  return inches\n",
        "\n",
        "\n",
        "def find_roots(x,y):\n",
        "  s = np.abs(np.diff(np.sign(y))).astype(bool)\n",
        "  return x[:-1][s] + np.diff(x)[s]/(np.abs(y[1:][s]/y[:-1][s])+1)\n",
        "\n",
        "\n",
        "# This bit of code to subset the grib and only download things we want is\n",
        "# shamelessly stolen from Brian Blaylock (https://github.com/blaylockbk)\n",
        "def download_subset(remote_url, remote_file, local_filename):\n",
        "  print(\"   > Downloading a subset of NBM gribs\")\n",
        "  local_file = \"nbm/\"+local_filename\n",
        "  if \"qmd\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day max fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day min fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour min fcst:'\n",
        "    elif element == \"qpf\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':APCP:surface:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day acc fcst:'\n",
        "      else:\n",
        "        search_string = f':APCP:surface:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour acc fcst:'\n",
        "  elif \"core\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      search_string = f':TMAX:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      search_string = f':TMIN:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour min fcst:'\n",
        "  #print(search_string)\n",
        "  idx = remote_url+\".idx\"\n",
        "  r = requests.get(idx)\n",
        "  if not r.ok:\n",
        "    print('     ❌ SORRY! Status Code:', r.status_code, r.reason)\n",
        "    print(f'      ❌ It does not look like the index file exists: {idx}')\n",
        "\n",
        "  lines = r.text.split('\\n')\n",
        "  expr = re.compile(search_string)\n",
        "  byte_ranges = {}\n",
        "  for n, line in enumerate(lines, start=1):\n",
        "      # n is the line number (starting from 1) so that when we call for\n",
        "      # `lines[n]` it will give us the next line. (Clear as mud??)\n",
        "\n",
        "      # Use the compiled regular expression to search the line\n",
        "      if expr.search(line):\n",
        "          # aka, if the line contains the string we are looking for...\n",
        "\n",
        "          # Get the beginning byte in the line we found\n",
        "          parts = line.split(':')\n",
        "          rangestart = int(parts[1])\n",
        "\n",
        "          # Get the beginning byte in the next line...\n",
        "          if n+1 < len(lines):\n",
        "              # ...if there is a next line\n",
        "              parts = lines[n].split(':')\n",
        "              rangeend = int(parts[1])\n",
        "          else:\n",
        "              # ...if there isn't a next line, then go to the end of the file.\n",
        "              rangeend = ''\n",
        "\n",
        "          # Store the byte-range string in our dictionary,\n",
        "          # and keep the line information too so we can refer back to it.\n",
        "          byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
        "          #print(line)\n",
        "  for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
        "\n",
        "        if i == 0:\n",
        "            # If we are working on the first item, overwrite the existing file.\n",
        "            curl = f'curl -s --range {byteRange} {remote_url} > {local_file}'\n",
        "        else:\n",
        "            # If we are working on not the first item, append the existing file.\n",
        "            curl = f'curl -s --range {byteRange} {remote_url} >> {local_file}'\n",
        "        try:\n",
        "          num, byte, date, var, level, forecast, _ = line.split(':')\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        #print(f'  Downloading GRIB line [{num:>3}]: variable={var}, level={level}, forecast={forecast}')\n",
        "        os.system(curl)\n",
        "\n",
        "  if os.path.exists(local_file):\n",
        "      print(f'      ✅ Success! Searched for [{search_string}] and got [{len(byte_ranges)}] GRIB fields and saved as {local_file}')\n",
        "      return local_file\n",
        "  else:\n",
        "      print(print(f'      ❌ Unsuccessful! Searched for [{search_string}] and did not find anything!'))\n",
        "\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# This section for downloading and processing obs                                                                      #\n",
        "########################################################################################################################\n",
        "print('Getting obs...')\n",
        "obs = {}\n",
        "for region in region_list:\n",
        "  if (valid_end_datetime <= current_datetime):\n",
        "    print(\"   > Grabbing obs for: \", region)\n",
        "    #print(\"List of CWAs: \", cwa_list(region) )\n",
        "    json_name = \"obs/Obs_\"+element+\"_\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour+\"_\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour+\"_\"+region+\".json\"\n",
        "    if os.path.exists(\"obs\"):\n",
        "      pass\n",
        "    else:\n",
        "      !mkdir -p obs\n",
        "\n",
        "    if element != \"qpf\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=air_temp\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      api_extras = \"&units=temp%7Cf&within=1440&status=active\"\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query + api_extras\n",
        "      print(obs_url)\n",
        "    elif element == \"qpf\":\n",
        "      if use_stageiv:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        stageIV, s4xs, s4ys = get_stageiv()\n",
        "        s4xs, s4ys = np.meshgrid(s4xs, s4ys)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "      print(obs_url)\n",
        "    if os.path.exists(json_name):\n",
        "      pass\n",
        "    else:\n",
        "      urlretrieve(obs_url, json_name)\n",
        "      print(obs_url)\n",
        "\n",
        "    if os.path.exists(json_name):\n",
        "        with open(json_name) as json_file:\n",
        "            obs_json = json.load(json_file)\n",
        "            obs_lats = []\n",
        "            obs_lons = []\n",
        "            obs_value = []\n",
        "            obs_elev = []\n",
        "            obs_stid = []\n",
        "            obs_name = []\n",
        "            for stn in obs_json[\"STATION\"]:\n",
        "                # print(stn.encode('utf-8'))\n",
        "                if stn[\"STID\"] is None:\n",
        "                  stid = \"N0N3\"\n",
        "                else:\n",
        "                  stid = stn[\"STID\"]\n",
        "                #print(f'Processing {region} station {stid}')\n",
        "                name = stn[\"NAME\"]\n",
        "                if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "                  elev = stn[\"ELEVATION\"]\n",
        "                else:\n",
        "                  elev = -999\n",
        "                lat = stn[\"LATITUDE\"]\n",
        "                lon = stn[\"LONGITUDE\"]\n",
        "                if element == \"mint\" or element==\"maxt\":\n",
        "                  if 'air_temp_set_1' in stn['STATISTICS'] and stn['STATISTICS']['air_temp_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['air_temp_set_1'] and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['air_temp_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(float(stat))\n",
        "                elif (element == \"qpf\"):\n",
        "                  if (stn[\"STATUS\"] == \"ACTIVE\") and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_stageiv:\n",
        "                      coords = project_hrap(lon, lat, s4xs, s4ys)\n",
        "                      siv_value = float(stageIV[coords])\n",
        "                      if (siv_value >= 0.01):\n",
        "                        obs_value.append(siv_value)\n",
        "                      else:\n",
        "                        obs_value.append(np.NaN)\n",
        "                    else:\n",
        "                      if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                        if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                          ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                          if ptotal >= 0.01:\n",
        "                            obs_value.append(ptotal)\n",
        "                          else:\n",
        "                            obs_value.append(np.nan)\n",
        "                        else:\n",
        "                          obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(np.nan)\n",
        "\n",
        "\n",
        "\n",
        "            csv_name = \"obs_\"+element+\"_\"+region+\".csv\"\n",
        "            obs[region] = pd.DataFrame()\n",
        "            obs[region][\"stid\"] = obs_stid\n",
        "            obs[region][\"name\"] = obs_name\n",
        "            obs[region][\"elevation\"] = obs_elev\n",
        "            obs[region][\"lat\"] = obs_lats\n",
        "            obs[region][\"lon\"] = obs_lons\n",
        "            obs[region][\"ob_\"+element] = obs_value\n",
        "            #obs[region].to_csv(csv_name)\n",
        "\n",
        "        # In order to make elevation subsetting work for multiple iterations\n",
        "        os.remove(json_name)\n",
        "  else:\n",
        "    print(f'    > Valid Time in the future. Grabbing obs points only for: {region}')\n",
        "    json_name = \"obs/ObsPoints_\"+region+\".json\"\n",
        "    if os.path.exists(json_name):\n",
        "      pass\n",
        "    else:\n",
        "      if os.path.exists(\"obs\"):\n",
        "        pass\n",
        "      else:\n",
        "        !mkdir -p obs\n",
        "      obs_url = \"https://api.synopticdata.com/v2/stations/metadata?&token=\"+synoptic_token+\"&cwa=\"+cwa_list(region)+\"&fields=status,latitude,longitude,name,elevation\"+network_string\n",
        "      urlretrieve(obs_url, json_name)\n",
        "      print(obs_url)\n",
        "    if os.path.exists(json_name):\n",
        "      with open(json_name) as json_file:\n",
        "          obs_json = json.load(json_file)\n",
        "          obs_lats = []\n",
        "          obs_lons = []\n",
        "          obs_elev = []\n",
        "          obs_stid = []\n",
        "          obs_name = []\n",
        "          for stn in obs_json[\"STATION\"]:\n",
        "            # print(stn.encode('utf-8'))\n",
        "            if stn[\"STID\"] is None:\n",
        "              stid = \"N0N3\"\n",
        "            else:\n",
        "              stid = stn[\"STID\"]\n",
        "            #print(f'Processing {region} station {stid}')\n",
        "            name = stn[\"NAME\"]\n",
        "            if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "              elev = stn[\"ELEVATION\"]\n",
        "            else:\n",
        "              elev = -999\n",
        "            lat = stn[\"LATITUDE\"]\n",
        "            lon = stn[\"LONGITUDE\"]\n",
        "            if stn[\"STATUS\"] == \"ACTIVE\" and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "              obs_stid.append(str(stid))\n",
        "              obs_name.append(str(name))\n",
        "              obs_elev.append(float(elev))\n",
        "              obs_lats.append(float(lat))\n",
        "              obs_lons.append(float(lon))\n",
        "          obs[region] = pd.DataFrame()\n",
        "          obs[region][\"stid\"] = obs_stid\n",
        "          obs[region][\"name\"] = obs_name\n",
        "          obs[region][\"elevation\"] = obs_elev\n",
        "          obs[region][\"lat\"] = obs_lats\n",
        "          obs[region][\"lon\"] = obs_lons\n",
        "          obs[region][\"ob_\"+element] = -999\n",
        "          #obs[region].to_csv(csv_name)\n",
        "\n",
        "    # In order to make elevation subsetting work for multiple iterations\n",
        "    os.remove(json_name)\n",
        "\n",
        "for region in region_list:\n",
        "    querystr = f'{elev_lower_limit} <= elevation <= {elev_upper_limit}'\n",
        "    print(f'Subsetting obs for region {region} by query: {querystr}')\n",
        "    obs[region] = obs[region].query(querystr)\n",
        "\n",
        "if region_selection == 'RFC':\n",
        "\n",
        "  obs = pd.concat(obs).reset_index(\n",
        "    ).rename(columns={\"level_0\":\"cwa\"}\n",
        "    ).drop(columns=[\"level_1\"]).set_index('stid')\n",
        "\n",
        "  # Subset the indices to match\n",
        "  obs_set = set(obs.index.unique())\n",
        "  rfc_set = set(rfc_metadata.index.get_level_values(1).unique())\n",
        "  rfc_site_index = np.array(list(obs_set.intersection(rfc_set)), dtype=str)\n",
        "  obs = obs[obs.index.isin(rfc_site_index)]\n",
        "\n",
        "  obs = {cwa_selection:obs}\n",
        "\n",
        "  #Reset the region list\n",
        "  region_list = [cwa_selection]\n",
        "\n",
        "########################################################################################################################\n",
        "# This section downloads and processes the NBM.                                                                        #\n",
        "########################################################################################################################\n",
        "print('Getting and processing NBM...')\n",
        "nbm_init_filen = nbm_init.strftime('%Y%m%d') + \"_\" + nbm_init.strftime('%H')\n",
        "nbm_init_filen_core = core_init.strftime('%Y%m%d') + \"_\" + core_init.strftime('%H')\n",
        "nbm_url_base = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "            +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "nbm_url_base_core = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+core_init.strftime('%Y%m%d') \\\n",
        "            +\"/\"+core_init.strftime('%H')+\"/\"\n",
        "temp_vars = [\"maxt\",\"mint\"]\n",
        "if (element == \"qpf\"):\n",
        "  detr_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.co.grib2'\n",
        "  detr_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.co.{element}_subset.grib2'\n",
        "  detr_url = nbm_url_base+\"qmd/\"+detr_file\n",
        "\n",
        "elif any(te in element for te in temp_vars):\n",
        "  detr_file = f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.co.grib2\"\n",
        "  detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.co.{element}_subset.grib2\"\n",
        "  detr_url = nbm_url_base_core+\"core/\"+detr_file\n",
        "\n",
        "\n",
        "if os.path.exists(\"nbm/\"+detr_file_subset):\n",
        "  print(\"   > NBM deterministic already exists\")\n",
        "else:\n",
        "  print(\"   > Getting NBM deterministic\")\n",
        "  if os.path.exists(\"nbm\"):\n",
        "    pass\n",
        "  else:\n",
        "    !mkdir -p nbm\n",
        "  #urlretrieve(detr_url, \"nbm/\"+detr_file)\n",
        "  download_subset(detr_url, detr_file, detr_file_subset)\n",
        "#print(detr_url)\n",
        "nbmd = pygrib.open(\"nbm/\"+detr_file_subset)\n",
        "if element == \"maxt\":\n",
        "  deterministic = nbmd.select(name=\"Maximum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"max\")[0]\n",
        "  deterministic_array = K_to_F(deterministic.values)\n",
        "elif element == \"mint\":\n",
        "  deterministic = nbmd.select(name=\"Minimum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"min\")[0]\n",
        "  deterministic_array = K_to_F(deterministic.values)\n",
        "elif element == \"qpf\":\n",
        "  deterministic = nbmd.select(name=\"Total Precipitation\",lengthOfTimeRange=24)[-1]\n",
        "  deterministic_array = mm_to_in(deterministic.values)\n",
        "nbmlats, nbmlons = deterministic.latlons()\n",
        "nbmd.close()\n",
        "\n",
        "for region in region_list:\n",
        "  print(\"     >> Extracting NBM deterministic\")\n",
        "  point_lats = obs[region][\"lat\"].values\n",
        "  point_lons = obs[region][\"lon\"].values\n",
        "  detr_values = []\n",
        "  nbm_fidx = []\n",
        "  for i in range(0, len(point_lats)):\n",
        "    coords = ll_to_index(nbmlons, nbmlats, point_lons[i], point_lats[i])\n",
        "    detr_value = deterministic_array[coords]\n",
        "    nbm_fidx.append(coords)\n",
        "    detr_values.append(detr_value)\n",
        "  obs[region][\"NBM_fidx\"] = nbm_fidx\n",
        "  obs[region][\"NBM_D\"] = detr_values\n",
        "\n",
        "\n",
        "perc_list = [1,5,10,20,30,40,50,60,70,80,90,95,99]\n",
        "perc_list = np.array([i for i in range(1,100,1)])\n",
        "perc_dict = {\"maxt\":\"maxt18p\", \"mint\":\"mint18p\", \"qpf\":\"qpf24p\"}\n",
        "perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.co.grib2'\n",
        "perc_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.co.{element}_subset.grib2'\n",
        "perc_url = nbm_url_base+\"qmd/\"+perc_file\n",
        "if os.path.exists(\"nbm/\"+perc_file_subset):\n",
        "  print(\"   > NBM probabilistic already exists\")\n",
        "else:\n",
        "  #urlretrieve(perc_url, \"nbm/\"+perc_file)\n",
        "  print(\"   > Getting NBM probabilistic\")\n",
        "  download_subset(perc_url, perc_file, perc_file_subset)\n",
        "\n",
        "nbmperc = pygrib.open(\"nbm/\"+perc_file_subset)\n",
        "print('   > Extracting NBM Probabilistic')\n",
        "for perc in perc_list:\n",
        "  print(f'     >> Extracting NBM P{int(perc):01}')\n",
        "  perc_name = f\"NBM_P{perc:02d}\"#\"NBM_P\"+str(perc)\n",
        "\n",
        "  if element == \"maxt\":\n",
        "    percdata = K_to_F(nbmperc.select(name=\"Maximum temperature at 2 metres since previous post-processing\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "  elif element == \"mint\":\n",
        "    percdata = K_to_F(nbmperc.select(name=\"Minimum temperature at 2 metres since previous post-processing\", stepTypeInternal=\"min\", percentileValue=perc)[0].values)\n",
        "  elif element == \"qpf\":\n",
        "    percdata = mm_to_in(nbmperc.select(name=\"Total Precipitation\",lengthOfTimeRange=24, percentileValue=perc)[0].values)\n",
        "\n",
        "  for region in region_list:\n",
        "    nbm_coords = obs[region][\"NBM_fidx\"].values\n",
        "    perc_values = []\n",
        "    for i in range(0, len(nbm_coords)):\n",
        "      perc_value = percdata[nbm_coords[i]]\n",
        "      perc_values.append(perc_value)\n",
        "    obs[region][perc_name] = perc_values\n",
        "nbmperc.close()\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# This section creates a distribution curve at each site, and interpolates ob and deterministic to percentile space    #\n",
        "# 3/27/2024 MW Removed spline interpolation in favor of explicit percentiles. Left spline and debug code if needed.    #\n",
        "########################################################################################################################\n",
        "\n",
        "print('Creating point distribution curves and interpolating...')\n",
        "perc_list = np.array(perc_list)\n",
        "\n",
        "for region in region_list:\n",
        "\n",
        "  obs[region] = obs[region].sort_index(axis=1)\n",
        "\n",
        "  perc_start = obs[region].columns.get_loc(\"NBM_P01\")\n",
        "  perc_end = obs[region].columns.get_loc(\"NBM_P99\")\n",
        "  all_percs = obs[region].iloc[:, perc_start:perc_end+1].values\n",
        "  var_string = \"ob_\"+element\n",
        "  all_obs = obs[region][[var_string]].values\n",
        "  all_nbmd = obs[region][['NBM_D']].values\n",
        "  obs_percs = []\n",
        "  nbmd_percs = []\n",
        "\n",
        "  for i in range(0,len(all_obs)):\n",
        "\n",
        "    # udf = us(perc_list, all_percs[i,:], bbox=[0,100], ext=0)\n",
        "\n",
        "    if all_obs[i] <= all_percs[i,:][0]:#udf(0):\n",
        "      ob_perc = 0\n",
        "    elif all_obs[i] >= all_percs[i,:][-1]:#udf(100):\n",
        "      ob_perc = 100\n",
        "    else:\n",
        "    #   ob_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_obs[i])\n",
        "      ob_perc = find_roots(perc_list, all_percs[i,:] - all_obs[i])\n",
        "      ob_perc = ob_perc[0].round(1)\n",
        "\n",
        "    #   print('x: percentile list\\n', np.arange(0,101,1), end='\\n\\n')\n",
        "    #   print('udf(x)\\n', udf(np.arange(0,101,1)), end='\\n\\n')\n",
        "    #   print('all_obs[i]\\n', all_obs[i], end='\\n\\n')\n",
        "    #   print('y: udf(x) - ob value\\n', udf(np.arange(0,101,1)) - all_obs[i], end='\\n\\n')\n",
        "\n",
        "    # print('perc, perc_value, perc_value_interp')\n",
        "    # for pp, pv in zip(perc_list, all_percs[i,:]):\n",
        "    #     print(pp, pv, udf(pp))\n",
        "\n",
        "    # print()\n",
        "    # print('ob, ob_perc')\n",
        "    # print(all_obs[i], ob_perc)\n",
        "\n",
        "    if all_nbmd[i] <= all_percs[i,:][0]:#udf(0):\n",
        "      nbm_perc = 0\n",
        "    elif all_nbmd[i] >= all_percs[i,:][-1]:# udf(100):\n",
        "      nbm_perc = 100\n",
        "    else:\n",
        "    #   nbm_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_nbmd[i])\n",
        "      nbm_perc = find_roots(perc_list, all_percs[i,:] - all_nbmd[i])\n",
        "      nbm_perc = nbm_perc[0].round(1)\n",
        "\n",
        "    # print('nbm_det, nbm_perc')\n",
        "    # print(all_nbmd[i], nbm_perc)\n",
        "\n",
        "    if np.isnan(ob_perc):\n",
        "      obs_percs.append(ob_perc)\n",
        "    else:\n",
        "      obs_percs.append(int(ob_perc))\n",
        "\n",
        "    nbmd_percs.append(int(nbm_perc))\n",
        "\n",
        "    # print()\n",
        "    # print('ranked against raw percentiles (coarse-step10)')\n",
        "    perc_list=np.array(perc_list)\n",
        "    # print(find_roots(perc_list, all_percs[i,:]-all_obs[i]))\n",
        "\n",
        "\n",
        "  obs[region][\"ob_perc\"] = obs_percs\n",
        "  obs[region][\"NBMd_perc\"] = nbmd_percs\n",
        "\n",
        "  if export_csv:\n",
        "    csv_name = \"obs_\"+element+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\"_\"+region+\".csv\"\n",
        "    obs[region].to_csv(csv_name)\n",
        "    print(f'   > Created and saved {csv_name}')\n",
        "\n",
        "########################################################################################################################\n",
        "# Finally, this section makes our plot.                                                                                #\n",
        "########################################################################################################################\n",
        "def resize_colab_cell():\n",
        "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
        "get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
        "\n",
        "reg_cwa = region if region_selection != \"CWA\" else cwa_selection\n",
        "\n",
        "if (valid_end_datetime <= current_datetime):\n",
        "    compare_plots = ['obs', 'deterministic']\n",
        "else:\n",
        "    compare_plots = ['deterministic']\n",
        "\n",
        "for compare_to in compare_plots:\n",
        "\n",
        "    print(\"Making plot (almost done!)...\")\n",
        "    if compare_to ==\"obs\":\n",
        "        compare_var = \"ob_perc\"\n",
        "        compare_element = \"Obs\"\n",
        "    elif compare_to == \"deterministic\":\n",
        "        compare_var = \"NBMd_perc\"\n",
        "\n",
        "        if element ==\"qpf\":\n",
        "            compare_element = \"Detr\"\n",
        "        else:\n",
        "            compare_element = \"Detr\"\n",
        "\n",
        "    title_dict = {\"maxt\":[\"Max T\",\"PMaxT\"],\"mint\":[\"Min T\",\"PMinT\"], \"qpf\":[\"QPF\",\"PQPF\"]}\n",
        "\n",
        "    if (element == \"qpf\"):\n",
        "        valid_datetime = valid_date\n",
        "        fig_valid_date = valid_datetime.strftime('%Y%m%d_%HZ')\n",
        "        valid_title = valid_datetime.strftime('%HZ %a %m-%d-%Y')\n",
        "    else:\n",
        "        valid_datetime = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "        fig_valid_date = valid_datetime.strftime('%Y%m%d')\n",
        "        valid_title = valid_datetime.strftime('%a %m-%d-%Y')\n",
        "    nbm_init_title = nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "\n",
        "    if ((region_selection == \"CWA\") or (region_selection == \"RFC\")):\n",
        "        dataframeid = cwa_selection\n",
        "    else:\n",
        "        dataframeid = region_selection\n",
        "\n",
        "    mean = obs[dataframeid][compare_var].mean()\n",
        "    median = obs[dataframeid][compare_var].median()\n",
        "    mode = obs[dataframeid][compare_var].mode().values[0]\n",
        "\n",
        "    # Create a subplot figure with 1 row and 2 columns\n",
        "    fig = make_subplots(rows=1, cols=2,\n",
        "                        column_widths=[0.5, 0.5],  # Adjust size ratio between map and histogram\n",
        "                        specs=[[{\"type\": \"xy\"}, {\"type\": \"scattermapbox\"}]])\n",
        "\n",
        "    # Histogram plot\n",
        "\n",
        "    counts, bins = np.histogram(obs[dataframeid][compare_var], bins=range(0, 101, 10))\n",
        "    bins = 0.5 * (bins[:-1] + bins[1:])\n",
        "    hist_fig = px.bar(x=bins, y=counts, labels={'x':'Percentile Rank', 'y':'Counts'},\n",
        "                    opacity=0.7) #, text_auto=True)\n",
        "    fig.update_traces(marker_color='aqua')\n",
        "\n",
        "    # Kernel Density Estimation (KDE)\n",
        "    kde_x = np.linspace(0, 100, 1000)\n",
        "    kde_y = stats.gaussian_kde(obs[dataframeid][compare_var].dropna())(kde_x)\n",
        "\n",
        "    # Add KDE line to the same subplot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=kde_x, y=kde_y * np.max(counts) / np.max(kde_y),\n",
        "                mode='lines', name='KDE', showlegend=False,\n",
        "                line=dict(color='aqua', width=2)),\n",
        "        row=1, col=1)\n",
        "\n",
        "    # Add histogram trace to subplot\n",
        "    for trace in hist_fig.data:\n",
        "        fig.add_trace(trace, row=1, col=1)\n",
        "\n",
        "    # Add vertical lines for mean and median\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[mean, mean], y=[0, np.max(counts)],\n",
        "                mode='lines', name=f'Mean ({int(mean)})', line=dict(color='red', dash='dash', width=3)),\n",
        "        row=1, col=1)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[median, median], y=[0, np.max(counts)],\n",
        "                mode='lines', name=f'Median ({int(median)})', line=dict(color='green', width=3)),\n",
        "        row=1, col=1)\n",
        "\n",
        "    # Set the x-axis for the histogram on the left to always show 0 to 100\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(range=[0, 100]),  # Set x-axis range from 0 to 100\n",
        "        xaxis2=dict(range=[0, 100]), # Also apply it to the histogram subplot\n",
        "        bargap=0,  # Remove space between bars\n",
        "    )\n",
        "\n",
        "    obs[dataframeid]['size'] = 30\n",
        "    obs[dataframeid][f'{compare_var}_str'] = obs[dataframeid][compare_var].round(0).astype(str)\n",
        "    #hacky fix for now\n",
        "    obs[dataframeid][f'{compare_var}_str'] = [x.replace('.0', '') for x in obs[dataframeid][f'{compare_var}_str']]\n",
        "\n",
        "    # Scatter mapbox plot with explicit range_color and coloraxis\n",
        "    map_fig = px.scatter_mapbox(obs[dataframeid].dropna(), lat=\"lat\", lon=\"lon\",\n",
        "                                color=compare_var, opacity=0.9,\n",
        "                                size='size',\n",
        "                                mapbox_style=\"open-street-map\",\n",
        "                                color_continuous_scale=cmap, range_color=[0, 100],  # Force color range from 0 to 100\n",
        "                                text=f'{compare_var}_str',\n",
        "                                hover_data={\n",
        "                                    'name': True,  # Display the name of the point\n",
        "                                    'stid': True,  # Display the station ID\n",
        "                                    'elevation': True,\n",
        "                                    f'{compare_var}': True,  # Display the observed percentile rank\n",
        "                                    'NBMd_perc': True,  # Display the NBM percentile rank\n",
        "                                    'NBM_D': True,\n",
        "                                    f'{\"ob_\"+element}':True,\n",
        "                                    f'{compare_var}_str': False,  # Display the observed percentile rank\n",
        "                                    'size': False,  # Hide size from hover\n",
        "                                    'lat': False,   # Hide latitude from hover\n",
        "                                    'lon': False,   # Hide longitude from hover\n",
        "                                })\n",
        "\n",
        "    map_fig.update_traces(textposition='middle center', textfont=dict(color=txcol, weight='bold'))\n",
        "\n",
        "    # Add scatter mapbox trace to subplot\n",
        "    for trace in map_fig.data:\n",
        "        fig.add_trace(trace, row=1, col=2)\n",
        "\n",
        "    # Update layout with coloraxis and color range\n",
        "    fig.update_layout(\n",
        "        width=1600,\n",
        "        height=800,\n",
        "        bargap=0,\n",
        "        mapbox=dict(\n",
        "            center={\"lat\": obs[dataframeid]['lat'].mean(), \"lon\": obs[dataframeid]['lon'].mean()},\n",
        "            zoom=6\n",
        "        ),\n",
        "        mapbox_style=\"open-street-map\",\n",
        "        coloraxis=dict(\n",
        "            colorscale=cmap,\n",
        "            colorbar=dict(\n",
        "                title=\"<b>Percentile Rank</b>\",\n",
        "                tickfont=dict(size=18),\n",
        "                tickvals=[0, 20, 40, 60, 80, 100],  # Ensure the colorbar ticks go from 0 to 100\n",
        "                ticktext=[\"0\", \"20\", \"40\", \"60\", \"80\", \"100\"],  # Matching labels\n",
        "            ),\n",
        "            cmin=0,  # Set minimum color scale value\n",
        "            cmax=100,  # Set maximum color scale value\n",
        "        ),\n",
        "        margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
        "        legend=dict(\n",
        "            x=0.01,  # Position the legend to the left of the plot\n",
        "            y=0.99,  # Align it to the top of the plot\n",
        "            traceorder='normal',\n",
        "            font=dict(size=18),\n",
        "            bordercolor=\"Black\",\n",
        "            borderwidth=2,\n",
        "        ),\n",
        "        title_text=f'<b>{reg_cwa} {title_dict[element][0]} {compare_element} in NBM {title_dict[element][1]} Percentile Space</b> <br> Valid: {valid_title}  |  NBM Init: {nbm_init_title}  |  Points: {points_str}',\n",
        "        title_x=0.5,  # title_y=0.98,\n",
        "        title_font=dict(size=18),\n",
        "        xaxis=dict(\n",
        "            tickfont=dict(size=18)\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            tickfont=dict(size=18)\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    fig.show(config={'scrollZoom':True})"
      ],
      "metadata": {
        "id": "rlAmuTYm3ODD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlv8Zr4QJtFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}